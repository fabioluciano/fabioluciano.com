---
title: "DORA: Métricas de Fluxo, Capacidades Invisíveis e o Que Realmente Sustenta a Entrega Contínua"
description: "DORA não mede produtividade. Mede sintomas. E o que realmente importa está nas 24 capacidades que ninguém lembra de implementar."
publishedAt: 2026-01-30
locale: pt
category: platform-engineering
tags:
  - developer-experience
  - platform-engineering
  - metricas
  - dora
draft: false
toc: true
comments: true
series: por-que-times-produtivos-fracassam
seriesOrder: 4
translationSlug: why-productive-teams-fail-04
---

import { Callout } from '@/components/mdx/Callout';
import { CompareColumns } from '@/components/mdx/CompareColumns';
import { ProsCons } from '@/components/mdx/ProsCons';
import { Tooltip } from '@/components/mdx/Glossary';
import { Accordion, AccordionItem } from '@/components/mdx/Accordion';
import { FootnoteRef, FootnoteList, FootnoteItem } from '@/components/mdx/Footnote';

DORA virou moeda corrente em conversas sobre DevOps e entrega de software. Suas quatro métricas — deployment frequency, lead time for changes, mean time to recovery e change failure rate — são citadas em apresentações, usadas para justificar investimentos e incluídas em painéis corporativos.

Mas pouquíssimas pessoas lembram que **DORA não é sobre métricas**. DORA é sobre capacidades.

<Callout type="warning" title="O equívoco fundamental">
  As métricas DORA são indicadores de resultado. Elas não dizem o que fazer. Elas sinalizam se o que você está fazendo está funcionando. O trabalho real está nas **24 capacidades técnicas e organizacionais** que sustentam essas métricas.
</Callout>

## O problema epistemológico das métricas

Antes de mergulharmos nas métricas e capacidades, vale questionar algo raramente discutido: **métricas descrevem realidade ou a criam?**

Quando o DORA categoriza times em "Elite", "High", "Medium" e "Low performers", está apenas observando padrões — ou está estabelecendo uma hierarquia que organizações passarão a perseguir? Quando deployment frequency vira indicador de sucesso, estamos medindo saúde organizacional — ou criando um jogo onde "mais rápido" se torna sinônimo de "melhor", independentemente do contexto?

<Callout type="warning" title="A premissa oculta">
  DORA parte de correlações estatísticas observadas em milhares de organizações. Mas correlação não é lei universal. O que funciona para a maioria pode não funcionar para você. E o que mede bem uma coisa não necessariamente mede outra.
</Callout>

Essa distinção importa porque, quando esquecemos que métricas são recortes — não verdades absolutas — começamos a tratá-las como objetivos em si mesmos. E aí o sistema para de perguntar "estamos resolvendo o problema certo?" e passa a perguntar apenas "estamos melhorando os números?".

<ProsCons
  client:load
  prosTitle="O que DORA assume"
  consTitle="O que frequentemente ignora"
  context="Premissas implícitas do framework DORA"
  variant="warning"
  pros={[
    "Contextos organizacionais são comparáveis",
    "Velocidade é virtude universal",
    "Métricas revelam verdade objetiva",
    "Melhoria é sempre mensurável"
  ]}
  cons={[
    { text: "Cada contexto tem suas restrições únicas", emphasis: true },
    { text: "Velocidade tem custo humano e técnico", emphasis: true },
    "Métricas criam incentivos e comportamentos",
    "Nem tudo valioso aparece em painéis"
  ]}
/>

Esse não é um argumento contra métricas — é um alerta sobre o que acontece quando paramos de questionar o que elas realmente significam. Com essa lente crítica estabelecida, podemos olhar para as quatro métricas DORA com mais nuance.

## As métricas DORA como sintomas

<Tooltip client:load term="DORA Metrics" definition="Conjunto de 4 métricas que avaliam a capacidade de entrega de software: frequência de deploy, lead time, tempo de recuperação e taxa de falhas em mudanças.">As quatro métricas DORA</Tooltip> funcionam como termômetros, não como tratamento.

**Por que DORA não mede produtividade?** Porque produtividade em software não é sobre velocidade de entrega — é sobre **valor gerado sustentavelmente**. DORA mede sintomas de um sistema de entrega saudável (fluxo rápido, baixo risco, recuperação ágil), mas não mede:

- Se o que está sendo entregue resolve problemas reais
- Se a arquitetura está evoluindo ou apodrecendo
- Se o time consegue sustentar esse ritmo sem quebrar
- Se há aprendizado ou apenas repetição mecânica
- Se decisões técnicas estão melhorando ou sendo postergadas

**Um time pode ter métricas DORA excelentes e ainda assim:**
- Estar entregando funcionalidades que ninguém usa
- Acumulando dívida técnica insustentável
- Queimando pessoas no processo
- Evitando problemas complexos em favor de entregas fáceis

<Callout type="danger" title="A confusão perigosa">
  DORA mede **saúde da pipeline**, não produtividade. Um sistema de entrega eficiente é condição necessária, mas não suficiente, para produtividade real. Confundir as duas coisas leva organizações a otimizar para velocidade enquanto destroem valor e pessoas.
</Callout>

<ProsCons
  client:load
  prosTitle="O que as métricas mostram"
  consTitle="O que elas não dizem"
  context="Limitações inerentes das 4 métricas DORA"
  variant="balanced"
  pros={[
    "Com que frequência código vai para produção",
    "Quanto tempo leva da commit ao deploy",
    "Quanto tempo para restaurar o serviço após incidente",
    "Quantas mudanças causam falhas"
  ]}
  cons={[
    "Por que a frequência está baixa",
    "Onde está o gargalo no processo",
    "Por que incidentes acontecem",
    "O que torna mudanças arriscadas"
  ]}
/>

### Frequência de Implantação (Deployment Frequency)

Mede com que frequência código vai para produção. Alta frequência sinaliza que a pipeline é confiável, o risco percebido é baixo e mudanças podem ser pequenas.

**Não mede:** Se essas entregas importam. Se o time está sustentável. Se há dívida técnica crescendo em paralelo.

### Tempo de Entrega para Mudanças (Lead Time for Changes)

Tempo desde o commit até o código rodando em produção. Lead time baixo indica pipeline enxuto, menos etapas manuais, menos burocracia.

**Não mede:** Qualidade das mudanças. Tempo de revisão de código. Carga cognitiva para realizar o trabalho.

### Tempo Médio de Recuperação (Mean Time to Recovery - MTTR)

Quanto tempo leva para restaurar o serviço após uma falha. MTTR baixo indica boa observabilidade, processos claros de resposta a incidentes e capacidade de rollback rápido.

**Não mede:** Por que o incidente aconteceu. Quantos incidentes poderiam ter sido evitados. Desgaste emocional do time.

### Taxa de Falha de Mudanças (Change Failure Rate)

Percentual de mudanças que causam degradação ou incidente em produção. Taxa baixa indica testes eficazes, ambientes estáveis e implantações seguras.

**Não mede:** Cobertura de testes. Qualidade de design. Decisões técnicas que aumentam ou reduzem fragilidade.

<Callout type="info" title="As métricas como conversa">
  As métricas DORA são úteis porque direcionam conversas. Elas forçam perguntas: por que demoramos tanto para deployar? Por que nossos implantações quebram? Mas as respostas não estão nas métricas — estão nas **capacidades**.
</Callout>

## As 24 capacidades que ninguém implementa

O verdadeiro trabalho da pesquisa DORA está em identificar as **capacidades técnicas e organizacionais** que diferenciam times de alta performance. Elas são agrupadas em cinco categorias:

### 1. Capacidades Técnicas

#### Controle de versão para tudo (Version control for everything)

Código, configurações, infraestrutura, scripts. Tudo versionado. Sem "mudanças manuais no servidor". Sem configurações perdidas. Rollback sempre possível.

#### Automação de implantação (Deployment automation)

Implantações automáticas, repetíveis e confiáveis. Sem passos manuais. Sem "tem que rodar esse script antes". O processo é o mesmo em dev, staging e produção.

#### Integração Contínua (Continuous Integration)

Commits frequentes, testes rodando automaticamente, retorno rápido. Se a compilação quebra, todos sabem imediatamente.

#### Desenvolvimento baseado em trunk (Trunk-based development)

Branches de curta duração. Merges frequentes. Menos conflitos. Integração contínua real, não "CI de fim de semana".

#### Automação de testes (Test automation)

Testes unitários, de integração e de contrato. Cobertura significativa, não apenas métrica cosmética. Confiança para mudar código.

#### Gestão de dados de teste (Test data management)

Dados de teste isolados, reproduzíveis e seguros. Ambientes de teste que realmente se parecem com produção.

#### Segurança desde o início (Shift left on security)

Segurança desde o início. Análise estática, revisão de dependências, segredos gerenciados corretamente. Não "depois a gente vê isso".

#### Entrega Contínua (Continuous Delivery)

Código sempre implantável. Implantação é decisão de negócio, não proeza técnica. Pipeline confiável o suficiente para implantação a qualquer momento.

#### Acoplamento fraco (Loose coupling)

Mudanças em um serviço não quebram outros. Contratos claros. Dependências explícitas. Evolução independente.

#### Arquitetura (Architecture)

Times conseguem testar, deployar e mudar seus sistemas sem coordenação excessiva com outros times. Arquitetura permite autonomia.

### 2. Capacidades de Processo

#### Feedback do cliente (Customer feedback)

Ciclos curtos de feedback. Validar hipóteses rapidamente. Aprender com usuários reais, não com especulações internas.

#### Fluxo de valor (Value stream)

Entender o fluxo completo: da ideia ao valor entregue. Identificar gargalos, desperdícios e etapas desnecessárias.

#### Trabalho em pequenos lotes (Work in small batches)

Mudanças pequenas, frequentes e incrementais. Menos risco, feedback mais rápido, menos retrabalho.

#### Experimentação do time (Team experimentation)

Times têm autonomia para testar ideias, mudar processos, aprender com erros. Melhoria não vem de cima, vem da prática.

### 3. Capacidades de Gestão

#### Aprovação de mudanças (Change approval)

Aprovações leves, baseadas em confiança e controles automatizados — não em comitês. Peer review, não gates burocráticos.

#### Monitoramento e observabilidade (Monitoring and observability)

Visibilidade do que está acontecendo em produção. Logs estruturados, métricas, traces. Debugging eficaz.

#### Notificação proativa (Proactive notification)

Problemas detectados antes de virar incidente. Alertas significativos, não ruído.

#### Gestão de mudanças de banco de dados (Database change management)

Mudanças de schema versionadas, testadas e deployadas como código. Sem scripts manuais, sem "espero que funcione".

#### Limites de trabalho em progresso (WIP limits)

Trabalho em progresso limitado. Foco em finalizar antes de começar novo trabalho. Throughput real, não ilusão de movimento.

#### Gestão visual (Visual management)

Estado do trabalho visível para o time. Dashboards, quadros, sinais claros de progresso e bloqueios.

### 4. Capacidades Culturais

#### Cultura organizacional de Westrum (Westrum organizational culture)

Cultura generativa: informação flui, colaboração é esperada, falhas são oportunidades de aprendizado, não de culpa.

#### Cultura de aprendizado (Learning culture)

Tempo e espaço para aprender. Compartilhar conhecimento. Investir em desenvolvimento técnico. Não apenas "entregar mais".

#### Satisfação no trabalho (Job satisfaction)

Trabalho significativo. Autonomia. Senso de progresso. Times que se sentem valorizados entregam melhor.

#### Liderança transformacional (Transformational leadership)

Liderança que inspira, desafia intelectualmente, apoia individualmente e encoraja inovação. Não comando-e-controle.

<Callout type="danger" title="Métricas sem capacidades">
  Implementar painéis DORA sem investir nas capacidades é cosmético. Os números podem até melhorar por um tempo — por esforço heroico, atalhos ou ilusão estatística. Mas não se sustentam.
</Callout>

## A crítica que ninguém faz: quando DORA vira dogma

Implementar as 24 capacidades é fundamental. Elas representam o trabalho real de construir sistemas de entrega sustentáveis. Mas há uma conversa que raramente acontece em salas de reunião, conferências ou artigos sobre DevOps: **e se as próprias métricas DORA estiverem nos conduzindo na direção errada?**

Não se trata de negar o valor do framework. DORA trouxe rigor e evidência para discussões que antes eram apenas anedóticas. Mas há um problema que cresce silenciosamente: quando métricas se tornam verdades absolutas, quando correlações viram leis universais, quando números substituem julgamento.

<Callout type="warning" title="A pergunta incômoda">
  DORA mede **capacidade de entrega**. Não mede se essas entregas fazem sentido, nem quanto custa sustentá-las, nem o que está sendo sacrificado para manter os números verdes.
</Callout>

### As cinco críticas estruturais

#### 1. Contexto-cegueira: mesmas métricas, realidades opostas

Duas equipes podem ter frequência de deploy idêntica — 10 implantações por dia — e estar vivendo realidades completamente opostas.

**Time A:** Deploy frequente porque construiu pipeline automatizada robusta, testes confiáveis e cultura de confiança. Mudanças pequenas, rollback automático, zero ansiedade.

**Time B:** Implantação frequente porque está sob pressão gerencial constante. Funcionalidades são quebradas em PRs artificialmente pequenos para "subir o número". Testes foram reduzidos para acelerar CI. Time vive em estado de alerta permanente.

**Ambos aparecem como "Elite performers" no painel.**

DORA não distingue capacidade construída de atalho perigoso. As métricas são cegas ao **como** e ao **por quê** — elas só enxergam o **quanto**.

<Callout type="danger" title="O que isso significa">
  Dois times com métricas idênticas podem estar em trajetórias opostas: um rumo à sustentabilidade, outro rumo ao colapso. E o DORA não diferencia.
</Callout>

#### 2. Manipulação reversa: otimizando na direção errada

Quando métricas viram metas, sistemas começam a jogar contra elas. Não por malícia, mas por dinâmica organizacional<FootnoteRef client:load id={2} />.

**Exemplos reais de manipulação de métricas:**

- **Deployment Frequency:** Funcionalidades são quebradas em dezenas de PRs minúsculos. Uma mudança que deveria ser atômica vira 15 implantações "para subir a frequência".

- **Lead Time:** Commits são feitos mas não mergeados até o último momento. PRs ficam em draft. O "lead time oficial" cai, mas o tempo real de trabalho continua o mesmo.

- **MTTR (Mean Time to Recovery):** Auto-rollback é configurado de forma agressiva. Qualquer erro vira rollback automático, contado como "recuperação rápida" — mesmo quando deveria ser investigado.

- **Change Failure Rate:** Incidentes são reclassificados como "manutenção planejada". Mudanças problemáticas são escondidas em implantações "normais". A taxa cai no dashboard, mas os problemas continuam acontecendo.

<ProsCons
  client:load
  prosTitle="O que a métrica mostra"
  consTitle="O que está realmente acontecendo"
  context="Manipulação: quando números melhoram mas a realidade piora"
  variant="warning"
  pros={[
    "Alta frequência de deploy",
    "Lead time baixíssimo",
    "Recuperação rápida",
    "Baixa taxa de falhas"
  ]}
  cons={[
    { text: "Features fragmentadas artificialmente", emphasis: true },
    { text: "Work in Progress oculto", emphasis: true },
    { text: "Problemas não investigados", emphasis: true },
    { text: "Incidentes mascarados", emphasis: true }
  ]}
/>

O sistema não está melhorando — está aprendendo a mentir para o indicador.

#### 3. O custo humano invisível da performance

Um time pode manter métricas DORA excelentes enquanto as pessoas quebram por dentro.

**Cenário real:** Time mantém status "Elite performer" por 8 meses consecutivos. Deployment frequency alta, lead time baixo, MTTR impressionante. Nas retrospectivas, tudo parece bem. Nos painéis, tudo está verde.

Então, em um único mês, três desenvolvedores-chave pedem demissão. Quando questionados, a resposta é uniforme: **exaustão**.

O que as métricas não mostraram:
- Trabalho constante fora do horário para manter a frequência
- Ansiedade crônica antes de cada deploy
- Falta de tempo para aprendizado ou melhoria técnica
- Dívida técnica acumulando em silêncio
- Decisões apressadas para "não quebrar o ritmo"

<Callout type="danger" title="DORA continua verde">
  As métricas não sabem — e não podem saber — se a alta performance é sustentável ou está sendo extraída através de desgaste humano.
</Callout>

Um time pode estar "performando bem" segundo DORA e simultaneamente caminhando para burnout coletivo. A métrica não mede custo cognitivo, emocional ou social.

#### 4. A falácia da velocidade universal

DORA assume implicitamente que mais rápido é sempre melhor. Mas essa premissa não é universal — ela é contextual.

**Exemplo: Sistema de compliance financeiro**

Segundo DORA, este time é "Low performer":
- Deployment frequency: 2x por mês
- Lead time: 3 semanas
- Change failure rate: ~5%

Mas o contexto revela outra história:
- Cada mudança passa por auditoria obrigatória
- Implantações exigem janela de manutenção aprovada
- Rollback não é trivial (banco de dados legado, contratos externos)
- O custo de falha não é reputacional — é regulatório

**Esse time está sendo cauteloso por boas razões.** Acelerar poderia aumentar risco inaceitável. "Low performer" no DORA não significa "time ruim" — significa apenas que velocidade não é o objetivo certo para esse sistema.

<Callout type="warning" title="Nem todo sistema precisa ser rápido">
  Para certos domínios — sistemas críticos de segurança, infraestrutura financeira, hardware embarcado — estabilidade, previsibilidade e análise cuidadosa importam mais que velocidade.
</Callout>

DORA não ajuda a decidir quando velocidade é o objetivo errado. E quando aplicado cegamente, pode penalizar times que estão fazendo exatamente o que deveriam fazer.

#### 5. A crítica epistemológica: correlação não é lei

A crítica mais profunda — e mais raramente articulada — é filosófica.

DORA nasce de pesquisa empírica: milhares de organizações foram observadas, padrões foram identificados, correlações foram estabelecidas. Times com alta frequência de implantação **tendem** a ter melhor performance geral. Times com baixo MTTR **tendem** a ser mais resilientes.

**Mas correlação não é causalidade. E tendência não é lei universal.**

Mesmo assim, DORA é frequentemente tratado como:
- Verdade científica absoluta
- Checklist de maturidade organizacional
- Régua moral para julgar times
- Justificativa inquestionável para decisões técnicas

Isso transforma um instrumento de observação em uma **ideologia de gestão**.

<Callout type="danger" title="Quando DORA deixa de ser ferramenta">
  Quando usado para ranking entre times, quando métricas viram metas de performance individual, quando números substituem conversas — DORA deixa de iluminar problemas e passa a **ser** o problema.
</Callout>

O framework descreve o que foi observado em contextos específicos. Ele não prescreve o que deve ser feito em todos os contextos. E definitivamente não é uma garantia de que "melhorar os números" significa melhorar o sistema.

### A frase-síntese da crítica

**DORA mede quão bem um time consegue mudar software — não se essas mudanças fazem sentido, nem quanto custa sustentá-las, nem o que está sendo sacrificado para mantê-las.**

É um termômetro, não um diagnóstico. E termômetros não dizem se a febre é sintoma de algo sério ou apenas o corpo lutando saudavelmente contra uma infecção.

## Por que acreditamos: Gartner, legitimidade e teatro científico

Se essas críticas são tão evidentes — contexto importa, métricas podem ser manipuladas, números não capturam desgaste humano — por que o DORA é tratado como verdade inquestionável em tantas organizações?

A resposta passa por um nome que raramente aparece em discussões técnicas, mas que exerce influência desproporcional sobre decisões corporativas: **Gartner**.

### O que o Gartner realmente é (e não é)

<Callout type="info" title="Definição honesta">
  O Gartner não é um órgão científico, não produz conhecimento experimental e não valida práticas de forma neutra. Ele é, fundamentalmente, **uma empresa de consultoria que vende redução de risco decisório percebido**.
</Callout>

Quando um CIO ou VP de Engenharia precisa justificar um investimento de milhões em transformação DevOps, o problema não é técnico — é político:

- "E se der errado?"
- "Como eu justifico isso para o board?"
- "Quem mais está fazendo isso?"
- "Como sei que não é modinha?"

Citar o Gartner resolve esse problema. Não porque Gartner descobriu uma verdade técnica que ninguém mais sabia. Mas porque ele oferece algo mais valioso para executivos: **cobertura reputacional**.

<ProsCons
  client:load
  prosTitle="O que Gartner parece ser"
  consTitle="O que Gartner realmente é"
  context="Percepção vs realidade do papel do Gartner"
  variant="warning"
  pros={[
    "Pesquisa científica independente",
    "Validação neutra de tecnologias",
    "Descoberta de melhores práticas",
    "Órgão certificador de qualidade"
  ]}
  cons={[
    { text: "Advisory corporativo", emphasis: true },
    "Mapeamento de consenso de mercado",
    "Organização de narrativas existentes",
    { text: "Seguro reputacional para executivos", emphasis: true }
  ]}
/>

O valor do Gartner não está em estar certo. **Está em ser defensável.**

Se uma iniciativa falha, mas foi baseada em recomendação Gartner, a falha vira "decisão alinhada ao mercado que não funcionou neste contexto". Se não tinha endosso Gartner e falha, vira "aposta arriscada de um líder imprudente".

### Magic Quadrant: medindo aceitabilidade, não qualidade

O produto mais conhecido do Gartner — o **Magic Quadrant** — é frequentemente interpretado como ranking de qualidade técnica. Não é.

Ele mede **aceitabilidade corporativa**: quão seguro é escolher essa ferramenta sem ser questionado? Quão amplamente adotada ela já é? Quão bem a empresa vendor se posiciona no mercado?

#### O que o Magic Quadrant realmente avalia

O Magic Quadrant classifica vendors em quatro quadrantes com base em dois eixos:

- **Ability to Execute** (capacidade de executar): tamanho da empresa, alcance de mercado, viabilidade financeira, suporte ao cliente
- **Completeness of Vision** (completude de visão): alinhamento com tendências de mercado, inovação percebida, estratégia de produto

Note o que **não** está sendo medido diretamente:
- Qualidade técnica da solução
- Facilidade de uso
- Adequação a contextos específicos
- Custo-benefício real
- Experiência do desenvolvedor

Um produto tecnicamente superior, mas de uma startup pequena, nunca chega ao quadrante "Leaders". Um produto medíocre de um vendor gigante tem vantagem estrutural.

#### Por que executivos confiam mesmo assim

Porque o Magic Quadrant resolve o problema deles: **reduzir ansiedade decisória**.

Escolher um "Leader" no Magic Quadrant significa:
- Decisão pode ser explicada em uma reunião de board
- Consultores externos validarão a escolha
- Outros executivos reconhecerão a marca
- Se falhar, a culpa é compartilhada com "o mercado"

Escolher uma ferramenta fora do Quadrant exige justificativa ativa. Escolher dentro do Quadrant é a escolha padrão — não precisa ser justificada, precisa ser **contestada** para não acontecer.

### Por que o Gartner recomenda DORA

Agora fica mais claro por que DORA aparece consistentemente em relatórios e recomendações Gartner. Não porque DORA seja infalível, mas porque ele tem três propriedades que executivos (e o Gartner) valorizam:

**1. É simples e comunicável**

Quatro métricas. Fácil de explicar em um slide. Fácil de comparar ao longo do tempo. Fácil de reportar para stakeholders não-técnicos.

**Isso é moeda de troca executiva.** Métricas complexas, nuanceadas e dependentes de contexto não cabem em reuniões de 30 minutos com o C-level. Simplicidade vende — mesmo quando reduz realidade a ponto de distorcê-la.

**2. Tem "lastro científico suficiente"**

DORA vem de pesquisas com milhares de organizações, relatórios anuais, linguagem acadêmica, correlações estatísticas. Isso cria **autoridade simbólica** — mesmo sem causalidade forte.

Para o Gartner, isso é suficiente para legitimar o uso. Não precisa ser perfeito. Precisa ser defensável.

**3. Reforça uma narrativa conveniente**

DORA sustenta uma história que executivos querem contar:

> "Times mais rápidos são times melhores. Vamos investir em automação, DevOps e transformação ágil para subir nossos números."

Essa narrativa:
- Justifica orçamento para ferramentas
- Cria sensação de progresso mensurável
- Permite comparação com competidores
- Alinha tecnologia com "eficiência" (palavra mágica do C-level)

O Gartner não cria essa narrativa do zero — ele a organiza, embala e valida.

<Callout type="danger" title="O ponto crítico">
  Quando o Gartner recomenda DORA, ele está fazendo isso como **framework de governança**, não como modelo explicativo completo da realidade. O problema é que essa distinção desaparece na implementação.
</Callout>

### O efeito colateral: de observação a controle

O que começa como:

> "Vamos observar nossa capacidade de entrega"

vira rapidamente:

> "Vamos gerir pessoas por esses números"

Nesse momento:
- Métricas viram metas de performance
- Correlação vira causa
- Observação vira controle
- Framework vira moral

E nem o Gartner nem o DORA foram desenhados para isso — mas o sistema corporativo incentiva exatamente esse uso.

### A frase que resume tudo

<Callout type="danger" title="A verdade sobre Gartner e DORA">
  **Gartner não vende verdade. Ele vende consenso defensável.**

  DORA entra nesse pacote como uma métrica "científica o bastante" para ser usada — e "simples o bastante" para ser mal utilizada.
</Callout>

Executivos recorrem ao Gartner não para descobrir o que é certo, mas para garantir que suas decisões sejam difíceis de contestar. DORA funciona perfeitamente nesse papel: oferece números claros, tem aparência de rigor e reforça narrativas que já estavam em andamento.

O problema não é que o Gartner recomenda DORA. O problema é que organizações tratam essa recomendação como validação científica, quando é, na verdade, mapeamento de consenso de mercado. E consenso não é sinônimo de sabedoria.

## Os limites estruturais do DORA

Depois de entender as críticas ao framework e o papel do Gartner em sua legitimização, fica mais fácil ver onde DORA termina — não por falha de design, mas por **limitações inerentes** a qualquer sistema de métricas.

DORA é poderoso para medir a **capacidade de entrega** de um sistema. Mas há perguntas fundamentais que ele não responde — e nunca se propôs a responder:

<ProsCons
  client:load
  prosTitle="O que DORA enxerga"
  consTitle="O que DORA não enxerga"
  context="Limites estruturais do framework"
  variant="balanced"
  pros={[
    "Velocidade da pipeline",
    "Estabilidade do deploy",
    "Capacidade de recuperação",
    "Frequência de mudanças"
  ]}
  cons={[
    { text: "Qualidade das decisões técnicas", emphasis: true },
    { text: "Sustentabilidade do ritmo", emphasis: true },
    { text: "Desgaste cognitivo do time", emphasis: true },
    "Impacto real para o usuário"
  ]}
/>

### O que falta no framework

- **Satisfação do time**: DORA não mede burnout, rotatividade ou carga cognitiva — apenas produção.
- **Qualidade de código**: Métricas não dizem se o código é manutenível, testável ou compreensível.
- **Valor de negócio**: Implantação frequente não garante que o que está sendo entregue resolve problemas reais.
- **Developer Experience**: Fricção, clareza, ferramentas e processos ficam completamente invisíveis.
- **Custo organizacional**: O esforço político, social e emocional para manter os números não aparece.
- **Contexto e escolhas**: DORA não ajuda a decidir se velocidade é o objetivo certo para seu sistema.

<Callout type="warning" title="O que acontece quando ignoramos os limites">
  Quando organizações tratam DORA como framework completo — em vez de uma lente entre várias — criam sistemas que otimizam para métricas enquanto desgastam pessoas, acumulam dívida técnica e produzem mudanças que ninguém pediu.
</Callout>

Essas lacunas não são bugs. São características inerentes de qualquer framework que tenta reduzir complexidade organizacional e humana a números comparáveis.

<Callout type="tip" title="DORA como ponto de partida, não de chegada">
  DORA funciona melhor como ponto de partida, não como verdade completa. Ele força conversas importantes sobre fluxo, capacidades e entrega contínua. Mas precisa ser complementado com outras lentes — SPACE, DevEx, conversas qualitativas com o time — para criar uma visão honesta do que está acontecendo.
</Callout>

## Capacidades primeiro, métricas depois — mas com os olhos abertos

Se há uma coisa que o trabalho DORA deixa claro, é que **métricas são consequência, não causa**. Mas há outra verdade, menos confortável: **métricas também são instrumentos políticos**, não apenas técnicos.

Você não melhora a frequência de implantação criando um dashboard. Você melhora investindo em automação, testes, trunk-based development e cultura de experimentação.

Você não reduz MTTR apenas pedindo que as pessoas sejam mais rápidas. Você reduz com observabilidade, runbooks claros, rollback automatizado e cultura blameless.

<Callout type="info" title="A ordem importa">
  Implemente as capacidades. Observe os resultados. Use as métricas para validar progresso, não para forçar comportamento. E sempre pergunte: **o que essas métricas não estão mostrando?**
</Callout>

### A verdade completa sobre DORA

DORA mede **quão bem um time consegue mudar software em produção**. Ele não mede:
- Se essas mudanças fazem sentido
- Quanto custa sustentá-las
- O que está sendo sacrificado para mantê-las

Isso não torna DORA inútil. Torna-o **incompleto** — e potencialmente perigoso quando usado como única lente.

As 24 capacidades não são checklist. São investimentos que competem por tempo, atenção e recursos com features, prazos e pressões de curto prazo. E nessa competição, métricas frequentemente vencem — porque são mais fáceis de medir do que valor real.

### Vivendo com DORA conscientemente

Se sua organização usa (ou exige) métricas DORA, você não precisa rejeitá-las. Mas precisa usá-las **conscientemente**:

**Pergunte sempre:**
- Estamos medindo porque queremos entender — ou porque precisamos reportar?
- Quem se beneficia se esses números melhorarem?
- O que ficaria invisível se olharmos apenas para as métricas?
- Nosso contexto realmente se beneficia de mais velocidade?

**Complemente sempre:**
- Métricas DORA com conversas regulares sobre desgaste e satisfação
- Dashboards quantitativos com observação qualitativa
- Números de entrega com percepção de valor de negócio
- Comparações externas com entendimento do próprio contexto

**E lembre-se:**

<Callout type="danger" title="A escolha que precede as métricas">
  Antes de escolher DORA (ou qualquer framework), escolha o problema que você quer resolver. Se essa escolha não for consciente, o sistema fará por você. E sistemas não se importam com burnout, contexto ou pessoas — eles otimizam para o que é medido.
</Callout>

**A pergunta não é se vale a pena investir em capacidades. A pergunta é: você está disposto a questionar as métricas que justificam (ou não) esse investimento?**

<FootnoteList>
  <FootnoteItem id={2}>
    A **Lei de Goodhart** afirma que "quando uma medida se torna uma meta, ela deixa de ser uma boa medida". Formulada pelo economista Charles Goodhart em 1975, essa lei descreve como sistemas se adaptam para otimizar métricas em vez de objetivos reais — fenômeno amplamente documentado em economia, ciências sociais e, cada vez mais, em engenharia de software.
  </FootnoteItem>
</FootnoteList>
